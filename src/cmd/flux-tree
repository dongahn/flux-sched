#!/bin/bash
#set -x

ORIG_FLUX_QMANAGER_OPTIONS=""
ORIG_FLUX_RESOURCE_OPTIONS=""
ORIG_FLUX_QMANAGER_RC_NOOP=""
ORIG_FLUX_RESOURCE_RC_NOOP=""

njobs=1
topo="1"
tgiven="no"
queue="default"
match="default"
prefix="tree"
out="%^+_no"
fxlogs="no"
leaf="no"

declare -r top_prefix="tree"
declare -i nnodes=1
declare -i ncores=1
declare -r t_delim="x"
declare -r p_delim=":"
declare -a mp_policies=( [low]=1, [high]=1, [locality]=1, [default]=1 )
declare -a qp_policies=( [fcfs]=1, [easy]=1, [hybrid]=1, \
[conservative]=1, [default]=1 )

declare -r long_opts="help,leaf,flux-logs,nnodes:,ncores-per-node:,"\
"topology:,queue-policy:,match-policy:,njobs:,perf-out:,prefix:"
declare -r short_opts="hlfN:C:T:Q:M:J:o:X:"

declare -r prog=${0##*/}
declare -r usage="
Usage: ${prog} [OPTIONS] -- Jobscript\n\
\n\
Create a Flux instance hierarchy according to the specified\n\
policies and schedule/run the specified number\n\
of Jobscripts at the last level of this hierarchy.\n\
\n\
If --topology=2x4 and --njobs=32 are given, for instance,\n\
2 Flux instances will be spawned from within the current instance,\n\
each of which will in turn spawn 4 child Flux instances, totaling\n\
8 instances at the last level of this hierarchy.\n\
Once this is done, 4 jobs (of Jobscripts) will be scheduled\n\
and executed at each of these 8 last-level Flux instances.\n\
\n\
The resources specified by --nnodes and --ncores-per-node\n\
are recursively divided such that each sibling Flux instance\n\
will be assigned to an equal split of the resources of their\n\
parent instance.\n\
\n\
Jobscript is expected to submit one or more programs through\n\
the flux-job submit command.\n\
Jobscript is passed with three additional environment variables:\n\
FLUX_TREE_ID, FLUX_TREE_LEAF_NNODES and FLUX_TREE_LEAF_NCORES_PER_NODE\n\
FLUX_TREE_ID is a string unique to each flux instance.\n\
FLUX_TREE_LEAF_NNODES is the number nodes assigned to the instance.\n\
FLUX_TREE_LEAF_NCORES_PER_NODE is the number of cores per node\n\
assigned to the instance.\n\
\n\
If --queue-policy and/or --match-policy are given, each level\n\
of this hierarchy will be set to the specified queuing and\n\
matching policies. Otherwise, all levels will be configured\n\
to be used either the default policies or policies specified\n\
through the FLUX_RESOURCE_OPTIONS and/or FLUX_QMANAGER_OPTIONS\n\
environment variables\n\
\n\
Options:\n\
 -h, --help                    Display this message\n\
 -l, --leaf                    Leaf (default=${leaf})\n\
 -f, --flux-logs               Dump Flux logs for all instances\n\
 -N, --nnodes=NNODES           Total num of nodes to use\n\
                                   (default=${nnodes})\n\
 -C, --ncores-per-node=NCORES  Total num of cores per node to use\n\
                                   (default=${ncores})\n\
 -T, --topology=HPOLICY        Topology of Flux instance hierarchy:\n\
                                   e.g., 2x2 (default=${topo})\n\
 -Q, --queue-policy=QPOLICY    Queuing policy for each level of\n\
                                   the hierarchy: e.g., easy:fcfs\n\
 -M, --match-policy=MPOLICY    Match policy for each level of\n\
                                   the hierarchy: e.g., low:high\n\
 -J, --njobs=NJOBS             Total num of Jobscripts to run\n\
                                   (default=${njobs})\n\
 -o, --perf-out=FILENAME       Dump the performance data into\n\
                                   the given file (default: don't print)\n\
"

die() { echo -e "$@"; exit 1; }

dump_analysis() {
    local blurb=${1}
    local out=${2}

    if [ "x${blurb}" = "x" ]
    then
        return
    fi

    local treeid=$(echo ${blurb} | jq ' .treeid ')
    local nnodes=$(echo ${blurb} | jq ' .my_nodes')
    local ncores=$(echo ${blurb} | jq ' .my_cores')
    local njobs=$(echo ${blurb} | jq ' .njobs')
    local elapse=$(echo ${blurb} | jq ' .perf.elapse')
    local match=$(echo ${blurb} | jq ' .perf.match')
    local begin=$(echo ${blurb} | jq ' .perf.begin')
    local end=$(echo ${blurb} | jq ' .perf.end')
    printf "%-25s %25s %25s %25s %35s %25s %15s %4s\n" \
${treeid} ${elapse} ${begin} ${end} ${match} ${njobs} \
${nnodes} ${ncores} >> ${out}
    local length=$(echo ${blurb} | jq " .child | length ")
    for i in `seq 0 $(( length - 1 ))`
    do
        dump_analysis "$(echo "${blurb}" | jq " .child[${i}] ")" ${level} ${out}
    done
}

dump_output() {
    local pref=${1}
    local blurb=${2}
    local out=${3}

    if [ ${out} != "%^+_no" ]
    then
        printf "%-25s %25s %25s %25s %35s %25s %15s %4s\n" \
"TreeID" "Elapse(sec)" "Begin(GTD)" "End(GTD)" "Match(usec)" \
"NJobs" "NNodes" "CPN" > ${out}
        dump_analysis "${blurb}" ${out}
    else
        if [ ${pref} != ${top_prefix} ]
        then
            FLUX_KVS_NAMESPACE=$(flux getattr parent-kvs-namespace) \
            FLUX_URI=$(flux getattr parent-uri) \
            flux kvs put -j "tree-perf"="${blurb}"
        fi
    fi
}

json() {
    local pref=${1}
    local njobs=${2}
    local nnodes=${3}
    local ncores=${4}
    local begin=${5}
    local end=${6}
    local avg=0
    local avail="yes"

    flux resource -h > /dev/null 2>&1 || avail="no"

    if [ "${avail}" = "yes" ]
    then
        avg=$(flux resource stat | grep "Avg" | awk '{print $4}')
    fi
    local elapse=$(python -c "print (${end}-${begin})")
    local el_match=$(python -c "print (${avg}*${njobs}*1000000.0)")
    local blurb="{\"treeid\":\"${pref}\",\"njobs\":\
${njobs},\"my_nodes\":${nnodes},\"my_cores\":${ncores},\"perf\":\
{\"begin\":${begin},\"end\":${end},\"elapse\":\
${elapse},\"match\":${el_match}}}"
    echo "${blurb}"
}

next_topo() {
    local topo=${1}
    local nx=""
    local token=""
    local level=`echo ${topo} | awk -F"${t_delim}" '{print NF}'`
    for i in `seq 2 ${level}`
    do
        token=$(echo ${topo} | cut -d"${t_delim}" -f${i})
        nx="${nx:+${nx}"${t_delim}"}${token}"
    done
    echo ${nx}
}

next_policy() {
    local policy=${1}
    local nx=""
    local level=`echo ${policy} | awk -F"${p_delim}" '{print NF}'`
    local token=""
    for i in `seq 2 ${level}`
    do
        token=$(echo ${policy} | cut -d"${p_delim}" -f${i})
        nx="${nx:+${nx}"${p_delim}"}${token}"
    done
    echo ${nx}
}

qpolicy_check() {
    local policies=${2}
    local level=`echo ${policies} | awk -F"${p_delim}" '{print NF}'`
    local token=""
    for i in `seq 1 ${level}`
    do
        token=$(echo ${policies} | cut -d"${p_delim}" -f${i})
        if [ ${qp_policies[${token}]:-"missing"} = "missing" ]
        then
            return 1
        fi
    done
    return 0
}

mpolicy_check() {
    local policies=${2}
    local level=`echo ${policies} | awk -F"${p_delim}" '{print NF}'`
    local token=""
    for i in `seq 1 ${level}`
    do
        token=$(echo ${policies} | cut -d"${p_delim}" -f${i})
        if [ ${mp_policies[${token}]:-"missing"} = "missing" ]
        then
            return 1
        fi
    done
    return 0
}

instance_policy() {
    local policy=${1}
    if [ ${policy} = "default" ]
    then
        echo "${policy}"
        return 0
    fi
    local token=$(echo ${policy} | cut -d"${p_delim}" -f1)
    echo ${token}
}

apply_policies() {
    local queue=${1}
    local match=${2}
    local queue_token=$(echo ${queue} | cut -d"${p_delim}" -f1)
    local match_token=$(echo ${match} | cut -d"${p_delim}" -f1)

    ORIG_FLUX_QMANAGER_OPTIONS=${FLUX_QMANAGER_OPTIONS:-none}
    ORIG_FLUX_RESOURCE_OPTIONS=${FLUX_RESOURCE_OPTIONS:-none}
    ORIG_FLUX_QMANAGER_RC_NOOP=${FLUX_QMANAGER_RC_NOOP:-none}
    ORIG_FLUX_RESOURCE_RC_NOOP=${FLUX_RESOURCE_RC_NOOP:-none}
    unset FLUX_QMANAGER_RC_NOOP
    unset FLUX_RESOURCE_RC_NOOP

    if [ ${queue_token} != "default" ]
    then
        export FLUX_QMANAGER_OPTIONS="queue-policy=${queue_token}"
    fi
    if [ ${match_token} != "default" ]
    then
        export FLUX_RESOURCE_OPTIONS="hwloc-whitelist=node,core,gpu \
policy=${match_token}"
    fi
}

unapply_policies() {
    local queue=${1}
    local match=${2}

    unset FLUX_QMANAGER_OPTIONS
    unset FLUX_RESOURCE_OPTIONS

    if [ ${ORIG_FLUX_QMANAGER_OPTIONS} != "none" ]
    then
        export FLUX_QMANAGER_OPTIONS=${ORIG_FLUX_QMANAGER_OPTIONS}
    fi
    if [ ${ORIG_FLUX_RESOURCE_OPTIONS} != "none" ]
    then
        export FLUX_RESOURCE_OPTIONS=${ORIG_FLUX_QMANAGER_OPTIONS}
    fi
    if [ ${ORIG_FLUX_QMANAGER_RC_NOOP} != "none" ]
    then
        export FLUX_QMANAGER_RC_NOOP=${ORIG_FLUX_QMANAGER_RC_NOOP}
    fi
    if [ ${ORIG_FLUX_RESOURCE_OPTIONS} != "none" ]
    then
        export FLUX_RESOURCE_RC_NOOP=${ORIG_FLUX_RESOURCE_RC_NOOP}
    fi
}


################################################################################
#                                                                              #
#                   Handle Leaf or Internal Flux Instances                     #
#                                                                              #
################################################################################

leaf() {
    local pref=${1}
    local my_nnodes=${2}
    local my_ncores=${3}
    local njobs=${4}
    local jobscript=${5}
    local out=${6}

    local begin=$(date +%s.%N)
    for job in `seq 1 ${njobs}`;
    do
        export FLUX_TREE_ID="${pref}.${rank}"
        export FLUX_TREE_LEAF_NNODES=${my_nnodes}
        export FLUX_TREE_LEAF_NCORES_PER_NODE=${my_ncores}
        eval "${jobscript}"
    done
    unset FLUX_TREE_ID
    unset FLUX_TREE_LEAF_NNODES
    unset FLUX_TREE_LEAF_NCORES_PER_NODE
    flux job drain
    local end=$(date +%s.%N)
    flux job undrain

    local blurb="$(json ${pref} ${njobs} ${nnodes} ${ncores} ${begin} ${end})"
    dump_output ${pref} "${blurb}" ${out}
}

# Note: If my_nodes does not evenly divide my_cores, we can end up
# idling some cores Similarly, minimum num of cores is 1,
# if Flux instance hierarchy is larger than the number of cores, we can
# end up overscheduling the cores.
internal() {
    local pref=${1}
    local topo=${2}
    local queue=${3}
    local match=${4}
    local nnodes=${5}
    local ncores=${6}
    local size=${7}
    local njobs=${8}
    local script=${9}
    local flogs=${10}
    local out=${11}
    local jobspec=""
    local adj_rank=0
    local my_cores=0
    local adj_my_cores_rem=0
    local my_nodes=0
    local total_cores=$(( nnodes*ncores ))
    local adj_rem=$(( size+total_cores%size ))
    local adj_njobs_rem=$(( size+njobs%size ))
    local adj_ncores=$(( ncores+1 ))
    local adj_nnodes=$(( nnodes+1 ))
    local nx=$(next_topo ${topo})
    local nx_queue_p=$(next_policy ${queue})
    local nx_match_p=$(next_policy ${match})
    local flogopt=""
    local jobid=""
    declare -a jobids
    nx=${nx:+"-T ${nx}"}
    nx=${nx:-"-leaf"}
    nx_queue_p=${nx_queue_p:+"-Q ${nx_queue_p}"}
    nx_match_p=${nx_match_p:+"-M ${nx_match_p}"}

    apply_policies ${queue} ${match}
    local begin=$(date +%s.%N)
    for rank in `seq 1 ${size}`; do
        if [ ${flogs} = "yes" ]
        then
            flogopt="-o,-S,log-filename=${pref}.${rank}.log.out"
        fi
        adj_rank=$(( size+rank ))
        my_cores=$(( total_cores/size+adj_rem/adj_rank ))
        if [ ${my_cores} -eq 0 ]
        then
           my_cores=1
        fi

        my_njobs=$(( njobs/size+adj_njobs_rem/adj_rank ))
        adj_my_cores_rem=$(( ncores+my_cores%ncores ))
        my_nodes=$(( my_cores/ncores+adj_my_cores_rem/adj_ncores ))
        my_cores=$(( my_cores/my_nodes ))

        jobid=`flux jobspec srun -N ${my_nodes} -n ${my_nodes} -c ${my_cores} \
flux start ${flogopt} flux tree -N ${my_nodes} -C ${my_cores} ${nx} \
${nx_match_p} ${nx_queue_p} -X ${pref}.${rank} ${script} -J ${my_njobs} \
${flogopt:+"-f"} | flux job submit -`
        jobids[${rank}]=${jobid}
    done
    flux job drain
    local end=$(date +%s.%N)

    flux job undrain
    unapply_policies ${queue} ${match}

    local blurb="$(json ${pref} ${size} ${nnodes} ${ncores} ${begin} ${end})"
    local child=""
    local kvspath=""
    for rank in `seq 1 ${size}`;
    do
        kvspath=$(flux job id --from=dec --to=kvs ${jobids[${rank}]})
        child=$(flux kvs get -j ${kvspath}.guest.tree-perf)
        rm -f ${pref}.${rank}.out
        blurb=$(echo "${blurb}" | jq " .child += [${child}]")
    done
    dump_output ${pref} "${blurb}" ${out}
}


################################################################################
#                                                                              #
#                                    Main                                      #
#                                                                              #
################################################################################

main() {
    local leaf=${1}
    local prefix=${2}
    local topo=${3}
    local queue=${4}
    local match=${5}
    local nnodes=${6}
    local ncores=${7}
    local njobs=${8}
    local script=${9}
    local flogs=${10}
    local out=${11}
    local size=0

    if [ ${leaf} = "yes"  ]
    then
        leaf ${prefix} ${nnodes} ${ncores} ${njobs} ${script} ${out}
    else
        size=$(echo ${topo} | awk -F"${t_delim}" '{print $1}')
        internal ${prefix} ${topo} ${queue} ${match} ${nnodes} \
${ncores} ${size} ${njobs} ${script} ${flogs} ${out}
    fi
}


################################################################################
#                                                                              #
#                             Script Level Main                                #
#                                                                              #
################################################################################

GETOPTS=`/usr/bin/getopt -u -o ${short_opts} -l ${long_opts} -n ${prog} -- ${@}`
eval set -- "${GETOPTS}"
rcopt=$?

while true; do
    case "${1}" in
      -h|--help)                   echo -ne "${usage}";          exit 0  ;;
      -l|--leaf)                   leaf="yes";                   shift 1 ;;
      -f|--flux-logs)              fxlogs="yes";                 shift 1 ;;
      -N|--nnodes)                 nnodes=${2};                  shift 2 ;;
      -C|--ncores-per-node)        ncores=${2};                  shift 2 ;;
      -T|--topology)               topo=${2}; tgiven="yes";      shift 2 ;;
      -Q|--queue-policy)           queue=${2};                   shift 2 ;;
      -M|--match-policy)           match=${2};                   shift 2 ;;
      -J|--njobs)                  njobs=${2};                   shift 2 ;;
      -o|--perf-out)               out=${2};                     shift 2 ;;
      -X|--prefix)                 prefix=${2};                  shift 2 ;;
      --)                          shift; break;                         ;;
      *)                           die "Invalid option '${1}'\n${usage}" ;;
    esac
done

script=${1}

if [ $# -ne 1 -o ${rcopt} -ne 0 ]
then
    die "${usage}"
fi
if [ ${nnodes} -le 0 -o ${ncores} -le 0 ]
then
    die "nnodes or ncores must be greater than 0!"
fi
if [ ${tgiven} = "yes" -a ${leaf} = "yes" ]
then
    die "--topo and --leaf are given at the same time!"
fi
if [ ! -x ${script} ]
then
    die "${script} does not exist or is not executable!"
fi

qpolicy_check "${qp_policies}" ${queue} || die "Invalid policy (${queue})"

mpolicy_check "${mp_policies}" ${match} || die "Invalid policy (${match})"

main ${leaf} ${prefix} ${topo} ${queue} ${match} \
${nnodes} ${ncores} ${njobs} ${script} ${fxlogs} ${out}
